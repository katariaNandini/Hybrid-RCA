"""
Advanced evaluation metrics for RCAgent LLM model
Implements semantic metrics including METEOR, NUBIA, BLEURT, BARTScore, and embedding-based scores
"""

import json
import re
import numpy as np
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass
import requests
import openai
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# Try to import advanced metrics (install with: pip install nltk rouge-score bert-score)
try:
    from nltk.translate.meteor_score import meteor_score
    from rouge_score import rouge_scorer
    import bert_score
    METEOR_AVAILABLE = True
    ROUGE_AVAILABLE = True
    BERT_SCORE_AVAILABLE = True
except ImportError:
    METEOR_AVAILABLE = False
    ROUGE_AVAILABLE = False
    BERT_SCORE_AVAILABLE = False
    print("Warning: Some advanced metrics not available. Install with: pip install nltk rouge-score bert-score")

@dataclass
class AdvancedEvaluationResult:
    """Container for advanced evaluation results"""
    meteor_score: float
    rouge_scores: Dict[str, float]
    bert_score: float
    embedding_score: float
    gpt4_correctness: float
    gpt4_helpfulness: float
    overall_score: float

class AdvancedRCAgentEvaluator:
    """Advanced evaluation framework using semantic metrics and GPT-4 scoring"""
    
    def __init__(self, openai_api_key: str = None, embedding_model: str = "all-MiniLM-L6-v2"):
        self.openai_api_key = openai_api_key
        self.embedding_model_name = embedding_model
        
        # Initialize embedding model
        try:
            self.embedding_model = SentenceTransformer(embedding_model)
        except Exception as e:
            print(f"Warning: Could not load embedding model {embedding_model}: {e}")
            self.embedding_model = None
        
        # Initialize OpenAI client if API key provided
        if openai_api_key:
            openai.api_key = openai_api_key
        
        # Initialize ROUGE scorer
        if ROUGE_AVAILABLE:
            self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    
    def evaluate_semantic_similarity(self, 
                                   predicted_text: str, 
                                   reference_text: str) -> Dict[str, float]:
        """
        Evaluate semantic similarity using multiple metrics
        
        Args:
            predicted_text: Text generated by the model
            reference_text: Ground truth reference text
            
        Returns:
            Dictionary of similarity scores
        """
        scores = {}
        
        # METEOR Score
        if METEOR_AVAILABLE:
            try:
                # METEOR requires tokenized text
                pred_tokens = predicted_text.lower().split()
                ref_tokens = reference_text.lower().split()
                scores['meteor'] = meteor_score([ref_tokens], pred_tokens)
            except Exception as e:
                print(f"METEOR calculation error: {e}")
                scores['meteor'] = 0.0
        else:
            scores['meteor'] = 0.0
        
        # ROUGE Scores
        if ROUGE_AVAILABLE:
            try:
                rouge_scores = self.rouge_scorer.score(reference_text, predicted_text)
                scores['rouge1'] = rouge_scores['rouge1'].fmeasure
                scores['rouge2'] = rouge_scores['rouge2'].fmeasure
                scores['rougeL'] = rouge_scores['rougeL'].fmeasure
            except Exception as e:
                print(f"ROUGE calculation error: {e}")
                scores['rouge1'] = scores['rouge2'] = scores['rougeL'] = 0.0
        else:
            scores['rouge1'] = scores['rouge2'] = scores['rougeL'] = 0.0
        
        # BERTScore
        if BERT_SCORE_AVAILABLE:
            try:
                P, R, F1 = bert_score.score([predicted_text], [reference_text], lang="en")
                scores['bert_score'] = F1.item()
            except Exception as e:
                print(f"BERTScore calculation error: {e}")
                scores['bert_score'] = 0.0
        else:
            scores['bert_score'] = 0.0
        
        # Embedding-based similarity
        scores['embedding_score'] = self._calculate_embedding_similarity(predicted_text, reference_text)
        
        return scores
    
    def evaluate_with_gpt4(self, 
                          predicted_root_cause: str,
                          predicted_recommendation: str,
                          ground_truth_root_cause: str,
                          ground_truth_recommendation: str,
                          telemetry_data: Dict[str, Any]) -> Dict[str, float]:
        """
        Evaluate using GPT-4 for correctness and helpfulness scoring
        
        Args:
            predicted_root_cause: Model's root cause prediction
            predicted_recommendation: Model's recommendation
            ground_truth_root_cause: Ground truth root cause
            ground_truth_recommendation: Ground truth recommendation
            telemetry_data: Telemetry data used for analysis
            
        Returns:
            Dictionary with GPT-4 scores
        """
        if not self.openai_api_key:
            return {"gpt4_correctness": 0.0, "gpt4_helpfulness": 0.0}
        
        try:
            # Create evaluation prompt
            prompt = self._create_gpt4_evaluation_prompt(
                predicted_root_cause, predicted_recommendation,
                ground_truth_root_cause, ground_truth_recommendation,
                telemetry_data
            )
            
            # Call GPT-4 API
            response = openai.ChatCompletion.create(
                model="gpt-4-0613",
                messages=[
                    {"role": "system", "content": "You are an expert evaluator for root cause analysis systems."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.0,  # Deterministic for reproducibility
                max_tokens=200
            )
            
            # Parse response
            response_text = response.choices[0].message.content
            scores = self._parse_gpt4_response(response_text)
            
            return scores
            
        except Exception as e:
            print(f"GPT-4 evaluation error: {e}")
            return {"gpt4_correctness": 0.0, "gpt4_helpfulness": 0.0}
    
    def evaluate_nubia_metrics(self, 
                             predicted_text: str, 
                             reference_text: str) -> Dict[str, float]:
        """
        Evaluate using NUBIA (6-dimensional) metrics
        
        NUBIA provides 6 dimensions of evaluation:
        1. Semantic similarity
        2. Grammatical correctness
        3. Coherence
        4. Fluency
        5. Relevance
        6. Overall quality
        """
        # Simplified NUBIA implementation (would need actual NUBIA library)
        scores = {}
        
        # Semantic similarity (using embedding similarity)
        scores['semantic_similarity'] = self._calculate_embedding_similarity(predicted_text, reference_text)
        
        # Grammatical correctness (simplified)
        scores['grammatical_correctness'] = self._evaluate_grammar(predicted_text)
        
        # Coherence (simplified)
        scores['coherence'] = self._evaluate_coherence(predicted_text)
        
        # Fluency (simplified)
        scores['fluency'] = self._evaluate_fluency(predicted_text)
        
        # Relevance (simplified)
        scores['relevance'] = self._evaluate_relevance(predicted_text, reference_text)
        
        # Overall quality (weighted average)
        scores['overall_quality'] = np.mean(list(scores.values()))
        
        return scores
    
    def evaluate_bart_score(self, 
                           predicted_text: str, 
                           reference_text: str) -> Dict[str, float]:
        """
        Evaluate using BARTScore metrics
        
        BARTScore provides F-Score and CNN/DM metrics
        """
        scores = {}
        
        # Simplified BARTScore implementation
        # In practice, you would use the actual BARTScore library
        
        # F-Score (precision and recall based)
        f_score = self._calculate_f_score(predicted_text, reference_text)
        scores['f_score'] = f_score
        
        # CNN/DM style evaluation
        cnn_dm_score = self._calculate_cnn_dm_score(predicted_text, reference_text)
        scores['cnn_dm'] = cnn_dm_score
        
        return scores
    
    def comprehensive_evaluation(self, 
                               predicted_root_cause: str,
                               predicted_recommendation: str,
                               ground_truth_root_cause: str,
                               ground_truth_recommendation: str,
                               telemetry_data: Dict[str, Any]) -> AdvancedEvaluationResult:
        """
        Run comprehensive evaluation using all advanced metrics
        """
        
        # Semantic similarity for root cause
        root_cause_scores = self.evaluate_semantic_similarity(predicted_root_cause, ground_truth_root_cause)
        
        # Semantic similarity for recommendation
        recommendation_scores = self.evaluate_semantic_similarity(predicted_recommendation, ground_truth_recommendation)
        
        # GPT-4 evaluation
        gpt4_scores = self.evaluate_with_gpt4(
            predicted_root_cause, predicted_recommendation,
            ground_truth_root_cause, ground_truth_recommendation,
            telemetry_data
        )
        
        # NUBIA metrics
        nubia_root_cause = self.evaluate_nubia_metrics(predicted_root_cause, ground_truth_root_cause)
        nubia_recommendation = self.evaluate_nubia_metrics(predicted_recommendation, ground_truth_recommendation)
        
        # BARTScore
        bart_root_cause = self.evaluate_bart_score(predicted_root_cause, ground_truth_root_cause)
        bart_recommendation = self.evaluate_bart_score(predicted_recommendation, ground_truth_recommendation)
        
        # Calculate overall scores
        meteor_score = (root_cause_scores['meteor'] + recommendation_scores['meteor']) / 2
        rouge_scores = {
            'rouge1': (root_cause_scores['rouge1'] + recommendation_scores['rouge1']) / 2,
            'rouge2': (root_cause_scores['rouge2'] + recommendation_scores['rouge2']) / 2,
            'rougeL': (root_cause_scores['rougeL'] + recommendation_scores['rougeL']) / 2
        }
        bert_score = (root_cause_scores['bert_score'] + recommendation_scores['bert_score']) / 2
        embedding_score = (root_cause_scores['embedding_score'] + recommendation_scores['embedding_score']) / 2
        
        # Overall score (weighted average)
        overall_score = (
            meteor_score * 0.2 +
            np.mean(list(rouge_scores.values())) * 0.2 +
            bert_score * 0.2 +
            embedding_score * 0.2 +
            (gpt4_scores['gpt4_correctness'] + gpt4_scores['gpt4_helpfulness']) / 20 * 0.2
        )
        
        return AdvancedEvaluationResult(
            meteor_score=meteor_score,
            rouge_scores=rouge_scores,
            bert_score=bert_score,
            embedding_score=embedding_score,
            gpt4_correctness=gpt4_scores['gpt4_correctness'],
            gpt4_helpfulness=gpt4_scores['gpt4_helpfulness'],
            overall_score=overall_score
        )
    
    # Helper methods
    def _calculate_embedding_similarity(self, text1: str, text2: str) -> float:
        """Calculate cosine similarity between text embeddings"""
        if not self.embedding_model:
            return 0.0
        
        try:
            embeddings = self.embedding_model.encode([text1, text2])
            similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
            return float(similarity)
        except Exception as e:
            print(f"Embedding similarity error: {e}")
            return 0.0
    
    def _create_gpt4_evaluation_prompt(self, 
                                     predicted_root_cause: str,
                                     predicted_recommendation: str,
                                     ground_truth_root_cause: str,
                                     ground_truth_recommendation: str,
                                     telemetry_data: Dict[str, Any]) -> str:
        """Create evaluation prompt for GPT-4"""
        
        prompt = f"""
Evaluate the following root cause analysis predictions:

TELEMETRY DATA:
{json.dumps(telemetry_data, indent=2)}

PREDICTED ANALYSIS:
Root Cause: {predicted_root_cause}
Recommendation: {predicted_recommendation}

GROUND TRUTH:
Root Cause: {ground_truth_root_cause}
Recommendation: {ground_truth_recommendation}

Please evaluate the predicted analysis on two dimensions:

1. G-Correctness (0-10): How accurate is the root cause identification?
   - Does it correctly identify the main issue?
   - Is it supported by the telemetry data?
   - Does it align with the ground truth?

2. G-Helpfulness (0-10): How helpful is the recommendation?
   - Is it actionable and specific?
   - Does it address the identified root cause?
   - Would it help resolve the issue?

Respond with only two numbers separated by a comma (e.g., "7,8"):
"""
        return prompt
    
    def _parse_gpt4_response(self, response_text: str) -> Dict[str, float]:
        """Parse GPT-4 response to extract scores"""
        try:
            # Extract numbers from response
            numbers = re.findall(r'\d+(?:\.\d+)?', response_text)
            if len(numbers) >= 2:
                return {
                    "gpt4_correctness": float(numbers[0]),
                    "gpt4_helpfulness": float(numbers[1])
                }
        except Exception as e:
            print(f"Error parsing GPT-4 response: {e}")
        
        return {"gpt4_correctness": 0.0, "gpt4_helpfulness": 0.0}
    
    def _evaluate_grammar(self, text: str) -> float:
        """Simplified grammatical correctness evaluation"""
        # Basic grammar checks
        score = 1.0
        
        # Check for basic grammar issues
        if text.count('.') == 0 and len(text) > 50:
            score -= 0.2  # Missing periods
        
        if text.count(',') == 0 and len(text) > 100:
            score -= 0.1  # No commas in long text
        
        # Check for repeated words
        words = text.lower().split()
        if len(words) > 10:
            word_counts = {}
            for word in words:
                word_counts[word] = word_counts.get(word, 0) + 1
            
            max_repetition = max(word_counts.values())
            if max_repetition > len(words) * 0.3:
                score -= 0.2  # Too much repetition
        
        return max(0.0, score)
    
    def _evaluate_coherence(self, text: str) -> float:
        """Simplified coherence evaluation"""
        # Basic coherence checks
        score = 1.0
        
        # Check for logical flow indicators
        transition_words = ['however', 'therefore', 'furthermore', 'moreover', 'additionally', 'consequently']
        has_transitions = any(word in text.lower() for word in transition_words)
        
        if has_transitions:
            score += 0.1
        
        # Check for topic consistency
        sentences = text.split('.')
        if len(sentences) > 1:
            # Simple topic consistency check
            first_sentence_words = set(sentences[0].lower().split())
            last_sentence_words = set(sentences[-1].lower().split())
            
            if first_sentence_words and last_sentence_words:
                overlap = len(first_sentence_words.intersection(last_sentence_words))
                if overlap > 0:
                    score += 0.1
        
        return min(1.0, score)
    
    def _evaluate_fluency(self, text: str) -> float:
        """Simplified fluency evaluation"""
        score = 1.0
        
        # Check for sentence length variation
        sentences = text.split('.')
        if len(sentences) > 2:
            sentence_lengths = [len(s.split()) for s in sentences if s.strip()]
            if sentence_lengths:
                length_variance = np.var(sentence_lengths)
                if length_variance > 0:
                    score += 0.1  # Good length variation
        
        # Check for appropriate word length
        words = text.split()
        if words:
            avg_word_length = np.mean([len(word) for word in words])
            if 4 <= avg_word_length <= 8:
                score += 0.1  # Good word length
        
        return min(1.0, score)
    
    def _evaluate_relevance(self, predicted_text: str, reference_text: str) -> float:
        """Evaluate relevance to reference text"""
        # Use embedding similarity as relevance measure
        return self._calculate_embedding_similarity(predicted_text, reference_text)
    
    def _calculate_f_score(self, predicted_text: str, reference_text: str) -> float:
        """Calculate F-Score between predicted and reference text"""
        pred_words = set(predicted_text.lower().split())
        ref_words = set(reference_text.lower().split())
        
        if not ref_words:
            return 0.0
        
        precision = len(pred_words.intersection(ref_words)) / len(pred_words) if pred_words else 0
        recall = len(pred_words.intersection(ref_words)) / len(ref_words)
        
        if precision + recall == 0:
            return 0.0
        
        f_score = 2 * (precision * recall) / (precision + recall)
        return f_score
    
    def _calculate_cnn_dm_score(self, predicted_text: str, reference_text: str) -> float:
        """Calculate CNN/DM style score"""
        # Simplified CNN/DM evaluation
        # In practice, you would use the actual CNN/DM evaluation metrics
        
        # Use ROUGE-L as a proxy for CNN/DM style evaluation
        if ROUGE_AVAILABLE:
            try:
                rouge_scores = self.rouge_scorer.score(reference_text, predicted_text)
                return rouge_scores['rougeL'].fmeasure
            except:
                pass
        
        # Fallback to simple word overlap
        pred_words = predicted_text.lower().split()
        ref_words = reference_text.lower().split()
        
        if not ref_words:
            return 0.0
        
        overlap = len(set(pred_words).intersection(set(ref_words)))
        return overlap / len(ref_words)

def run_advanced_evaluation_example():
    """Example of advanced evaluation"""
    
    # Example data
    predicted_root_cause = "High latency in trace_span.csv with p99 of 3167ms indicates database bottleneck"
    predicted_recommendation = "Optimize database queries and implement caching to reduce latency"
    
    ground_truth_root_cause = "Database performance issues causing high latency in trace operations"
    ground_truth_recommendation = "Optimize database queries and add caching mechanisms"
    
    telemetry_data = {
        "files": 4,
        "rows_total": 7143883,
        "error_events_total": 0,
        "worst_latency_p99": 3167.0,
        "worst_latency_file": "trace_span.csv"
    }
    
    # Initialize evaluator
    evaluator = AdvancedRCAgentEvaluator()
    
    # Run comprehensive evaluation
    result = evaluator.comprehensive_evaluation(
        predicted_root_cause, predicted_recommendation,
        ground_truth_root_cause, ground_truth_recommendation,
        telemetry_data
    )
    
    print("Advanced Evaluation Results:")
    print(f"METEOR Score: {result.meteor_score:.3f}")
    print(f"ROUGE-1: {result.rouge_scores['rouge1']:.3f}")
    print(f"ROUGE-2: {result.rouge_scores['rouge2']:.3f}")
    print(f"ROUGE-L: {result.rouge_scores['rougeL']:.3f}")
    print(f"BERT Score: {result.bert_score:.3f}")
    print(f"Embedding Score: {result.embedding_score:.3f}")
    print(f"GPT-4 Correctness: {result.gpt4_correctness:.1f}/10")
    print(f"GPT-4 Helpfulness: {result.gpt4_helpfulness:.1f}/10")
    print(f"Overall Score: {result.overall_score:.3f}")
    
    return result

if __name__ == "__main__":
    # Install required packages first:
    # pip install nltk rouge-score bert-score sentence-transformers openai scikit-learn
    
    run_advanced_evaluation_example()

